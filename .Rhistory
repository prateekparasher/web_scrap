knitr::opts_chunk$set(echo = TRUE)
```{r pressure, echo=FALSE}
```{r setup, include=FALSE}
install.packages("lpSolve")
library("lpSolve")
install.packages("rmarkdown")
library("rmarkdown")
obj.fun=c(5000,-2000) #Loading the objective function in obj.fun
constr=matrix(c(1,1,1,0,0,1),ncol = 2, byrow = TRUE) #Loading the constraints
constr.dir=c("=",">=",">=")
constr.rhs=c(200,80,100)
mod=lp("max",obj.fun,constr,constr.dir,constr.rhs,compute.sens = TRUE) #Using lp() to solve our problem
mod$solution #Displaying the values of x and y
x=100
y=100
z = (5000*x)-(2000*y) #Puting the values of x and y in the objective function
options("scipen"=100, "digits"=4)
cat("Net profit =", z) #Displaying the maximum profit
x=100
y=80
z = (5000*x)-(2000*y) #Puting the values of x and y in the objective function
options("scipen"=100, "digits"=4)
cat("Net profit =", z) #Displaying the maximum profit
library("lpSolve")
obj.fun=c(5000,-2000) #Loading the objective function in obj.fun
constr=matrix(c(1,1,1,0,0,1),ncol = 2, byrow = TRUE) #Loading the constraints
constr.dir=c("=",">=",">=")
constr.rhs=c(200,80,100)
mod=lp("max",obj.fun,constr,constr.dir,constr.rhs,compute.sens = TRUE) #Using lp() to solve our problem
mod$solution #Displaying the values of x and y
M=100
N=80
L = (5000*M)-(2000*N) #Puting the values of M and N in the objective function
options("scipen"=100, "digits"=4)
cat("Net profit =", L) #Displaying the maximum profit
data(arbuthnot)
library(dplyr)
install.packages("dplyr")
install.packages("ggplot2")
install.packages("statsr")
library(dplyr)
library(ggplot2)
library(statsr)
install.packages("devtools")
library(devtools)
install.packages("twitteR")
# package twitteR
library("twitteR")
key = "	hJd7F8u1RKv8aEMK6WbQy5Imi"
secret = "	G3i5O8D6e4PXeXmLFJkhp3KhmyG1K2GR4VPlFGq0Enbe19FESr"
# set a working directory for the whole process - you need to download a few files
# and R needs to know where to look for that stuff
setwd("D:/web_scrap")
# this is crucial step - at least for windows users
# if you are on Linux or Mac you might skip this step, Win needs the certificate collection
# Cacert.pem is a collection of certificates
download.file(url="http://curl.haxx.se/ca/cacert.pem",
destfile="D:/web_scrap",
method="auto")
# we are entering the whole Twitter API info and call the whole object authenticate
authenticate <-  OAuthFactory$new(consumerKey=key,
consumerSecret=secret,
requestURL='https://api.twitter.com/oauth/request_token',
accessURL='https://api.twitter.com/oauth/access_token',
authURL='https://api.twitter.com/oauth/authorize')
install.packages("rjson")
install.packages(c('ROAuth','RCurl'))
require('ROAuth')
require('RCurl')
library("twitteR")
library("twitteR")
# all this info is obtained for the Twitter developer account
key = "	hJd7F8u1RKv8aEMK6WbQy5Imi"
secret = "	G3i5O8D6e4PXeXmLFJkhp3KhmyG1K2GR4VPlFGq0Enbe19FESr"
# set a working directory for the whole process - you need to download a few files
# and R needs to know where to look for that stuff
setwd("D:/web_scrap")
download.file(url="http://curl.haxx.se/ca/cacert.pem",
destfile="D:/web_scrap",
method="auto")
authenticate <-  OAuthFactory$new(consumerKey=key,
consumerSecret=secret,
requestURL='https://api.twitter.com/oauth/request_token',
accessURL='https://api.twitter.com/oauth/access_token',
authURL='https://api.twitter.com/oauth/authorize')
download.file(url="http://curl.haxx.se/ca/cacert.pem",
destfile="D:/web_scrap",
method="auto")
curl -kO https://cran.r-project.org/src/contrib/Cairo_1.5-8.tar.gz
R CMD INSTALL Cairo_1.5-8.tar.gz
curl -kO https://cran.r-project.org/src/contrib/Cairo_1.5-8.tar.gz
R CMD INSTALL Cairo_1.5-8.tar.gz
curl -kO https://cran.r-project.org/src/contrib/Cairo_1.5-8.tar.gz
R CMD INSTALL Cairo_1.5-8.tar.gz
authenticate$handshake(cainfo="D:/web_scrap/cacert.pem")
authenticate <-  OAuthFactory$new(consumerKey=key,
consumerSecret=secret,
requestURL='https://api.twitter.com/oauth/request_token',
accessURL='https://api.twitter.com/oauth/access_token',
authURL='https://api.twitter.com/oauth/authorize')
authenticate <-  OAuthFactory$new(consumerKey=key,
consumerSecret=secret,
requestURL='https://api.twitter.com/oauth/request_token',
accessURL='https://api.twitter.com/oauth/access_token',
authURL='https://api.twitter.com/oauth/authorize')
# this will get you to a Twitter Site - obtain the PIN
# the whole process is meant to provide the signature for your Twitter usage
authenticate$handshake(cainfo="D:/web_scrap/cacert.pem")
library(rvest)
library(tidyverse)
install.packages('rvest')
#Loading the rvest package
library('rvest')
#Specifying the url for desired website to be scrapped
url <- 'https://weather.com/weather/tenday/l/EIXX6443:1:EI'
#Reading the HTML code from the website
webpage <- read_html(url)
#Using CSS selectors to scrap the rankings section
data_html <- html_nodes(webpage,'.text-primary')
install.packages("rvest")
head(rank_data)
rank_data <- html_text(rank_data_html)
install.packages("rvest")
library(rvest)
rank_data <- html_text(rank_data_html)
#Specifying the url for desired website to be scrapped
url <- 'https://weather.com/weather/tenday/l/EIXX6443:1:EI'
#Reading the HTML code from the website
webpage <- read_html(url)
#Using CSS selectors to scrap the rankings section
data_html <- html_nodes(webpage,'.text-primary')
#Converting the ranking data to text
rank_data <- html_text(rank_data_html)
#Specifying the url for desired website to be scrapped
url <- 'https://weather.com/weather/tenday/l/EIXX6443:1:EI'
#Reading the HTML code from the website
webpage <- read_html(url)
#Using CSS selectors to scrap the rankings section
data_html <- html_nodes(webpage,'.text-primary')
#Converting the ranking data to text
rank_data <- html_text(rank_data_html, trim=trim)
#Let's have a look at the rankings
head(rank_data)
#Data-Preprocessing: Converting rankings to numerical
rank_data<-as.numeric(rank_data)
#Let's have another look at the rankings
head(rank_data)
install.packages("rvest")
install.packages("rvest")
install.packages("rvest")
library(rvest)
url <- https://twitter.com/weatherrte?lang=en
web_page <- read_html(url)
head(web_page)
str(web_page)
install.packages("rvest")
library(rvest)
url <- https://twitter.com/weatherrte?lang=en
web_page <- read_html(url)
head(web_page)
str(web_page)
irish_warning <- html_nodes(web_page,'.tweet-text')
head(irish_warning, 30)
length(irish_warning)
data <- html_text(irish_warning)
head(data, 30)
install.packages("rvest")
url <- https://twitter.com/weatherrte?lang=en
web_page <- read_html(url)
head(web_page)
str(web_page)
irish_warning <- html_nodes(web_page,'.tweet-text')
head(irish_warning, 30)
length(irish_warning)
data <- html_text(irish_warning)
head(data, 30)
